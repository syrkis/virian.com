---
authors:
  - name: Noah Syrkis
    email: noah@virian.org
    institution: The Virian Project
  - name: Sophia De Spiegeleire
    email: noah@virian.org
    institution: Copenhagen University
title: Multimodal Brain Encoding
description: Utilising convolutional graph neural networks to reconstruct images
  from brain activity, this project spans neuroscience, geometric deep learning,
  and network analysis.
  Brain decoding holds immense promise for alleviation of neurological
  disorders.
date: 2023-06-29T13:50:44.015Z
illustration: /images/img_1170.jpeg
category: code
---
Understanding how the brain encodes visual information is a key challenge in neuroscience. In this project, we attempt to address this challenge by constructing a multimodal encoding model based on the Algonauts Project 2023 dataset. In addition to the dataset's image modality, we incorporate a semantic feature vector that describes object categories contained in the image shown to the subject during the functional Magnetic Resonance Imaging (fMRI) data collection. We combine various linear modules to construct two models: one predicting the fMRI data from both the associated image and the image's associated semantic feature vector; the other predicting both the fMRI data and the semantic vector from the image alone. Bayesian hyperparameter optimization suggests that the latter approach could potentially enhance model performance during inference without increasing the number of parameters. The model's performance was evaluated using a 5-fold cross-validation strategy and the median Pearson correlation coefficient as the metric. The code for this project is accessible at github.com/syrkis/neuroscope, and training logs are available at wandb.ai/syrkis/neuroscope.

## Introduction

Visual processing is the principal modality through which we interact and decipher our environment. Over the years, substantial progress has been made in understanding how the brain processes visual information, with even surprising parallels observed between artificial and biological vision processing [Cite]. However, as reality can exhibit extraordinarily different visual fingerprints—from simple geometric shapes to complex landscapes and visual noise—any system capable of visual perception is necessarily complicated. Fully capturing this complexity and intricacy remains a challenge. It is this challenge that is the focus of the 2023 Algonauts Project^[[http://algonauts.csail.mit.edu/](http://algonauts.csail.mit.edu/)]. The Algonauts Project's 2023 dataset is based on the Natural Scenes Dataset (NSD), which couples images from the Common Objects in Context (COCO) dataset [@lin2015] with fMRI responses to those images from various participants.

Neuroimaging techniques like fMRI have facilitated valuable insights into the neural correlates of visual perception. However, the potential of these techniques has been somewhat constrained by computational model limitations and the expense and time required to collect large-scale fMRI datasets. Amid these challenges, deep learning has proven to be a powerful tool, that has facilitated a better understanding and emulation of human visual perception. Recent efforts to incorporate multimodality into deep learning models have opened promising avenues to bridge the gap between computational models and the brain's complexity.

The experiment presented here explores how an additional modality might contribute to developing a model of the brain's visual encoding system, _without_ a large increase in complexity/parameter count. The additional modality used here is a semantic feature vector, derived from the COCO dataset, describing the object categories contained in each image. The two models we developed are tasked with 1) predicting the brain response given the image and knowledge of what is in the image, and 2) predicting the brain response and the semantic contents of the image. The second model is there as an additional source of potential confirmation of the usefulness of adding a second modality. The first model is the heart of our experiment, allowing us to explore the question: "_How does the inclusion of an additional modality have on the performance of a brain encoding model during inference?_" It is to attempt an answer to this question that we have constructed the two models and their corresponding unimodal baselines.

## Literature review

#### Visual information processing

Visual information processing, characterized by its hierarchical nature and intricate interconnectivity, plays a vital role in our understanding of the brain and perception. Traditionally, the process is categorized into low, mid, and high-level processing, focusing respectively on elementary visual features, their conjunctions, and abstract representations (@gonzalez-casillas2018,@groen2017). However, the hierarchical categorization is insufficient to capture the full complexity of real-world scene perception. It underrepresents the multimodal and interconnected nature of visual perception, particularly when processing complex stimuli such as natural scenes (@allen2022, @groen2017). 

The development of fMRI has opened up the possibility to examine and visualize brain activity associated with visual perception in real-time(@allen2022, @haxby2001). However, despite these advances, the understanding of the intricate interconnectivity in visual perception, particularly for natural scenes, remains limited. Furthermore, it is increasingly evident that a holistic understanding of visual perception requires the acquisition and analysis of massive amounts of data (@chang2019, @allen2022). To meet this need for data-intensive analysis in the field of visual neuroscience, deep learning has emerged as a promising tool.

#### Deep learning through visual encoding models

The power of deep learning models in the field of neuroscience is attributed to their ability to process and learn about high volumes of data, their inherent flexibility, and their structure, which is inspired by mirroring the brain’s hierarchical organization ((@kriegeskorte2015); (@kell2018)). In this field, deep neural networks (DNNs), particularly convolutional neural networks (CNNs), have been extensively used to predict brain activity in response to visual stimuli, known as visual encoding models. Findings from several studies demonstrate that DNNs and CNNs can accurately predict neural responses to various visual stimuli ((khaligh-razavi2014), (yamins2014), (cichy2016)). For instance, Zheng et al. (2021) applied a convolutional recurrent neural network (CRNN) to decipher the computational elements of the retinal circuit involved in interpreting natural scenes. Findings showed that recurrent spatiotemporal receptive fields of ganglion cells were key in encoding dynamic visual scenes and that the inherent recurrence of the model enhanced the prediction of the neural response (@zheng2021). 


Despite the richness of information available in fMRI data, remarkably few studies have exploited deep learning encoding models to predict associated brain responses. One study, by Güçlü and van Gerven (2015), utilized a DNN trained to predict object categories of over a million natural images and then used it to predict fMRI BOLD responses to complex naturalistic stimuli. The study identified a gradient of increasing complexity in the ventral visual pathway and found that the receptive fields in this region are attuned to object categorization (güçlü2015). In another study, Zhang et al (2019), constructed a new visual encoding model that was based on a unique combination of techniques: it employed transfer learning to use a pre-trained Deep Neural Network (AlexNet) and used a non-linear mapping to translate visual features into brain activity. The researchers found that their model yielded significant predictions for over 20% of the voxels in the early visual area and resulted in the outperformance of conventional linear mapping models, offering a new approach to leverage pre-trained visual features in brain activity prediction (@zhang2019). Notably, these DNNbased models can even outperform traditional hand-engineered models ((guclu2015), (kell2018), (@zhang2019)), signifying a substantial leap forward in uncovering the complexity of visual information processing.